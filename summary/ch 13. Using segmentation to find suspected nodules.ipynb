{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# ch 13. Using segmentation to find suspected nodules\n",
    "\n",
    "---\n",
    "\n",
    "이번 챕터에서는 프로젝트에 세그멘테이션 모델을 추가한다.\n",
    "\n",
    "기존 모델은 사람이 직접 분류하고 답을 분류해놓은 데이터를 이용했었다. 세그멘테이션 모델이 있다면 이 과정까지 모델에 맡길 수 있다.\n",
    "\n",
    "\n",
    "### 세그멘테이션의 유형\n",
    "\n",
    "1. **시맨틱 세그멘테이션**(**Semantic Segmentation**)\n",
    "   - 이번 장에서 우리가 사용할 유형이다. 레이블을 사용해서 이미지의 개별 픽셀을 분류하는 방식으로 동작한다.\n",
    "2. **인스턴스 세그멘테이션**(**Instance Segmentation**)\n",
    "   - 이 방법은 사람이 여러 명 있는 사진 등을 구분할 때 사용한다. 사람1, 사람2와 같은 방식으로 동일한 객체가 많을 때 이를 구분하는게 필요할 때 사용한다.\n",
    "3. **객체 탐지**(**Object Detection**)\n",
    "   - 대표적인 모델로 YOLO 모델이 있다. 주어진 이미지에서 관심있는 대상에 박스를 쳐서 나타내준다.\n",
    "\n",
    "\n",
    "기존에 우리가 구현했었던 분류 모델은 주어진 이미지를 하나의 포인트로 집결시키고 이를 토대로 분류한다. 즉 귀, 눈, 코와 같은 고차원 개념의 특성을 찾아내서 무엇인지 분류해낸다. 이 때 다운샘플링을 거치면서 컨볼루션의 수용필드가 증가하면서 더 넓은 영역에서도 고차원의 특성을 찾아낼 수 있게 된다.\n",
    "\n",
    "만약 출력과 입력이 같은 크기여야 한다면 이와는 다른 모델을 사용해야 한다. 단순하게 생각하면 다운샘플링을 거치치 않는 연속적 컨볼루션 구성을 생각해볼 수 있다. 하지만 이렇게 할 경우 수용필드에 한계가 생긴다. 수용필드가 입력 픽셀과 동일한 출력 픽셀을 유지하면서 어떻게 출력 픽셀의 수용필드를 개선할 수 있을까?\n",
    "\n",
    "**업샘플링**(**Upsampling**)을 사용하면된다. 주어진 이미지의 해상도보다 높은 해상도의 이미지를 만들어내는 것이다."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 유넷(U-Net) 아키텍처\n",
    "\n",
    "유넷의 아키텍처는 아래 그림과 같다. 데이터는 먼저 좌상단에서 하단 중앙부로 이동하면서, 여러 컨볼루션 층과 다운 스케일을 거친다. 그 후 다시 오른쪽 위로 올라가면서 업스케일링과 컨볼루션 층을 거치고 전체 해상도로 돌아온다. 하지만 이렇게 되면 수렴 문제가 발생한다. 즉, 다운샘플링을 거치는 동안 공간 정보를 손실하기 때문에 사물의 경계부분의 정확한 위치를 인코딩해서 재구성하기가 어려워진다.\n",
    "\n",
    "이를 해결하기 위해 유넷은 스킵 커넥션을 사용한다. 이로 인해 뒷부분에서는 아래 부분에서 나온 업샘플링 결과와 스킵 커넥션을 거치는 출력을 동시에 받게 된다. 이 부분이 유넷의 핵심이다. 유넷은 두 데이터 중에서 가장 나은 데이터(정보)를 사용해서 연산한다.\n",
    "\n",
    "\n",
    "![image](https://user-images.githubusercontent.com/76675506/191262454-8b37e6ea-a570-4794-b803-f7c968cbfc67.png)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 오픈 소스를 활용하기\n",
    "\n",
    "---\n",
    "\n",
    "유넷은 jvanvugt/pytorch-unet에 공개된 오픈 소스를 활용해서 구현한다. 여기서 필요한 부분만 수정해서 사용한다.\n",
    "\n",
    "순정 모델에서 요소를 하나씩 제거하면서 결과를 실험해보는 걸 **제거 연구**(**Ablation study**)라고도 한다. (https://cumulu-s.tistory.com/8)\n",
    "\n",
    "코드 출처: https://github.com/jvanvugt/pytorch-unet/blob/master/unet.py"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class UNet(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels=1,\n",
    "        n_classes=2,\n",
    "        depth=5,\n",
    "        wf=6,\n",
    "        padding=False,\n",
    "        batch_norm=False,\n",
    "        up_mode='upconv',\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Implementation of\n",
    "        U-Net: Convolutional Networks for Biomedical Image Segmentation\n",
    "        (Ronneberger et al., 2015)\n",
    "        https://arxiv.org/abs/1505.04597\n",
    "        Using the default arguments will yield the exact version used\n",
    "        in the original paper\n",
    "        Args:\n",
    "            in_channels (int): number of input channels\n",
    "            n_classes (int): number of output channels\n",
    "            depth (int): depth of the network\n",
    "            wf (int): number of filters in the first layer is 2**wf\n",
    "            padding (bool): if True, apply padding such that the input shape\n",
    "                            is the same as the output.\n",
    "                            This may introduce artifacts\n",
    "            batch_norm (bool): Use BatchNorm after layers with an\n",
    "                               activation function\n",
    "            up_mode (str): one of 'upconv' or 'upsample'.\n",
    "                           'upconv' will use transposed convolutions for\n",
    "                           learned upsampling.\n",
    "                           'upsample' will use bilinear upsampling.\n",
    "        \"\"\"\n",
    "        super(UNet, self).__init__()\n",
    "        assert up_mode in ('upconv', 'upsample')\n",
    "        self.padding = padding\n",
    "        self.depth = depth\n",
    "        prev_channels = in_channels\n",
    "        self.down_path = nn.ModuleList()\n",
    "        for i in range(depth):\n",
    "            self.down_path.append(\n",
    "                UNetConvBlock(prev_channels, 2 ** (wf + i), padding, batch_norm)\n",
    "            )\n",
    "            prev_channels = 2 ** (wf + i)\n",
    "\n",
    "        self.up_path = nn.ModuleList()\n",
    "        for i in reversed(range(depth - 1)):\n",
    "            self.up_path.append(\n",
    "                UNetUpBlock(prev_channels, 2 ** (wf + i), up_mode, padding, batch_norm)\n",
    "            )\n",
    "            prev_channels = 2 ** (wf + i)\n",
    "\n",
    "        self.last = nn.Conv2d(prev_channels, n_classes, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        blocks = []\n",
    "        for i, down in enumerate(self.down_path):\n",
    "            x = down(x)\n",
    "            if i != len(self.down_path) - 1:\n",
    "                blocks.append(x)\n",
    "                x = F.max_pool2d(x, 2)\n",
    "\n",
    "        for i, up in enumerate(self.up_path):\n",
    "            x = up(x, blocks[-i - 1])\n",
    "\n",
    "        return self.last(x)\n",
    "\n",
    "\n",
    "class UNetConvBlock(nn.Module):\n",
    "    def __init__(self, in_size, out_size, padding, batch_norm):\n",
    "        super(UNetConvBlock, self).__init__()\n",
    "        block = []\n",
    "\n",
    "        block.append(nn.Conv2d(in_size, out_size, kernel_size=3, padding=int(padding)))\n",
    "        block.append(nn.ReLU())\n",
    "        if batch_norm:\n",
    "            block.append(nn.BatchNorm2d(out_size))\n",
    "\n",
    "        block.append(nn.Conv2d(out_size, out_size, kernel_size=3, padding=int(padding)))\n",
    "        block.append(nn.ReLU())\n",
    "        if batch_norm:\n",
    "            block.append(nn.BatchNorm2d(out_size))\n",
    "\n",
    "        self.block = nn.Sequential(*block)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.block(x)\n",
    "        return out\n",
    "\n",
    "\n",
    "class UNetUpBlock(nn.Module):\n",
    "    def __init__(self, in_size, out_size, up_mode, padding, batch_norm):\n",
    "        super(UNetUpBlock, self).__init__()\n",
    "        if up_mode == 'upconv':\n",
    "            self.up = nn.ConvTranspose2d(in_size, out_size, kernel_size=2, stride=2)\n",
    "        elif up_mode == 'upsample':\n",
    "            self.up = nn.Sequential(\n",
    "                nn.Upsample(mode='bilinear', scale_factor=2),\n",
    "                nn.Conv2d(in_size, out_size, kernel_size=1),\n",
    "            )\n",
    "\n",
    "        self.conv_block = UNetConvBlock(in_size, out_size, padding, batch_norm)\n",
    "\n",
    "    def center_crop(self, layer, target_size):\n",
    "        _, _, layer_height, layer_width = layer.size()\n",
    "        diff_y = (layer_height - target_size[0]) // 2\n",
    "        diff_x = (layer_width - target_size[1]) // 2\n",
    "        return layer[\n",
    "            :, :, diff_y : (diff_y + target_size[0]), diff_x : (diff_x + target_size[1])\n",
    "        ]\n",
    "\n",
    "    def forward(self, x, bridge):\n",
    "        up = self.up(x)\n",
    "        crop1 = self.center_crop(bridge, up.shape[2:])\n",
    "        out = torch.cat([up, crop1], 1)\n",
    "        out = self.conv_block(out)\n",
    "\n",
    "        return out"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "아래 코드는 우리가 프로젝트에서 사용할 코드다. 먼저 입력 데이터를 배치 정규화한다. 이를 통해 데이터셋을 직접 정규화할 필요가 없어진다. 또한 개별 배치에 대해 통계량을 얻을 수 있다. 즉, 신경망에 들어간 크롭된 CT 데이터에서 아무것도 확인할 수 없을 때 비율을 조정할 수 있게 된다.\n",
    "\n",
    "\n",
    "그 후 출력을 `nn.Sigmoid`를 통해 범위를 [0,1]로 제한한다. 다음으로는 모델 자체의 전체 깊이와 필터수를 줄인다. 현재 모델은 우리가 사용할 데이터에 비해 capacity가 크기 때문이다."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# model.py:17, class UNetWrapper\n",
    "\n",
    "class UNetWrapper(nn.Module):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__()\n",
    "\n",
    "        self.input_batchnorm = nn.BatchNorm2d(kwargs['in_channels'])\n",
    "        self.unet = UNet(**kwargs)\n",
    "        self.final = nn.Sigmoid()\n",
    "\n",
    "        self._init_weights()\n",
    "\n",
    "    def forward(self, input_batch): # nn.sequential을 사용할 수 있지만 유지보수를 위해 명시적으로 작성했다.\n",
    "        bn_output = self.input_batchnorm(input_batch)\n",
    "        un_output = self.unet(bn_output)\n",
    "        fn_output = self.final(un_output)\n",
    "        return fn_output"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 세그멘테이션을 위한 데이터셋 업데이트\n",
    "\n",
    "---\n",
    "\n",
    "위 코드에서 `nn.BatchNorm2d`을 사용한다는 점에 주목하자. 유넷은 기본적으로 2차원 세그멘테이션 모델이다. 이를 사용하려면 우리 데이터(3차원)를 단면으로 나누고 한 번에 한 단면에 대해서 인접한 단면을 제공한다. 축 방향으로 존재하던 공간 정보(인접 관계)를 잃게 되지만, 모델이 이를 다시 학습하는건 어렵지 않다.\n",
    "\n",
    "\n",
    "또한, 유넷은 입출력 크기가 매우 제한적이다. CT 단면(512x512)보다 입력(572x572)은 크지만 출력(388x388)은 굉장히 작다. 이를 위해 유넷에 `padding` 파라미터를 `True`로 설정한다. 이로 인해 정확도가 떨어질 순 있지만 감수해야 하는 부분이다."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}