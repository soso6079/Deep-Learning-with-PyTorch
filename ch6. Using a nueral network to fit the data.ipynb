{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "Ch6. Using a nueral network to fit the data\n",
    "-------------------------------------------\n",
    "\n",
    "신경망의 기본 구조는 **뉴런**으로 이루어진 **계층**이다.\n",
    "뉴런은 입력에 대해 선형변환을 한 뒤 **활성 함수**(**Activation function**)라는 비선형 함수를 적용한다.\n",
    "\n",
    "이를 일반적인 수식으로 나타내면 $o =f(w * x + b)$으로 나타낼 수 있다. 각 변수가 스칼라 값이 아닌 벡터 값이면 여러 차원으로 가중치와 편향 값을 가진 여러 개의 뉴런으로 나타낸다. 이를 뉴런 **계층**이라고 한다."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "\n",
    "torch.set_printoptions(edgeitems=2, linewidth=75)\n",
    "t_c = [0.5,  14.0, 15.0, 28.0, 11.0,  8.0,  3.0, -4.0,  6.0, 13.0, 21.0]\n",
    "t_u = [35.7, 55.9, 58.2, 81.9, 56.3, 48.9, 33.9, 21.8, 48.4, 60.4, 68.4]\n",
    "t_c = torch.tensor(t_c).unsqueeze(1) # <1>\n",
    "t_u = torch.tensor(t_u).unsqueeze(1) # <1>\n",
    "\n",
    "n_samples = t_u.shape[0]\n",
    "n_val = int(0.2 * n_samples)\n",
    "\n",
    "shuffled_indices = torch.randperm(n_samples)\n",
    "\n",
    "train_indices = shuffled_indices[:-n_val]\n",
    "val_indices = shuffled_indices[-n_val:]\n",
    "\n",
    "t_u_train = t_u[train_indices]\n",
    "t_c_train = t_c[train_indices]\n",
    "\n",
    "t_u_val = t_u[val_indices]\n",
    "t_c_val = t_c[val_indices]\n",
    "\n",
    "t_un_train = 0.1 * t_u_train\n",
    "t_un_val = 0.1 * t_u_val"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "랜덤으로 train과 valid를 나누기 때문에 초기 가중치와 편향치가 책과 다를 수 있다."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[2.8406],\n        [0.5912]], grad_fn=<AddmmBackward0>)"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "linear_model = nn.Linear(1,1) # num of input feature, num of output feature, bias(defualt = True)\n",
    "linear_model(t_un_val)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "아래 두 줄은 결과적으로 같은 output이지만 `model`에 인자값을 넣고 호출하는 게 안전하다. `nn/modules/module.py`를 보면 `forward`를 진행하기 전에 수많은 `hook`을 진행한다."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[1;32m~\\AppData\\Local\\Temp\\ipykernel_20352\\3690513984.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[1;32m----> 1\u001B[1;33m \u001B[0my\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mmodel\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mx\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;31m# 권장\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m      2\u001B[0m \u001B[1;31m# y = model.forward(x)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mNameError\u001B[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "y = model(x) # 권장\n",
    "# y = model.forward(x)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "data": {
      "text/plain": "Parameter containing:\ntensor([[0.5827]], requires_grad=True)"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linear_model.weight"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "data": {
      "text/plain": "Parameter containing:\ntensor([-0.6792], requires_grad=True)"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linear_model.bias"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([-0.0964], grad_fn=<AddBackward0>)"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.ones(1)\n",
    "linear_model(x)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "위 코드에서 `linear_model`은 원하는 값을 뱉었다. 하지만 파이토치의 `nn.Module`은 다양한 샘플을 받기 위해  0번째 차원에 배치(batch)의 수가 들어있는걸 전제로 한다. $B \\times Nin$ 크기의 텐서를 넣어보자. $B$는 배치의 사이즈를 나타내고 $Nin$은 input feature의 수를 나타낸다."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[-0.0964],\n        [-0.0964],\n        [-0.0964],\n        [-0.0964],\n        [-0.0964],\n        [-0.0964],\n        [-0.0964],\n        [-0.0964],\n        [-0.0964],\n        [-0.0964]], grad_fn=<AddmmBackward0>)"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.ones(10, 1)\n",
    "linear_model(x)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "입력한 데이터의 배치 수와 동일한 결과의 수가 나왔다.\n",
    "\n",
    "> 배치를 사용하는 이유\n",
    "> - 연산 자원을 최대한 활용하기 위해\n",
    "> - (특정 모델의 경우) 전체 배치의 통계적 정보를 이용하기 때문. 배치의 사이즈가 클수록 더 좋은 결과를 얻는다.\n",
    "\n",
    "기존 온도 예제 데이터는 단순한 1차원 형태의 데이터였다. 이를 배치 형태($B \\times Nin$)로 바꾸기 위해 `unsqeeze`를 사용한다."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([11, 1])"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_c = [0.5,  14.0, 15.0, 28.0, 11.0,  8.0,  3.0, -4.0,  6.0, 13.0, 21.0]\n",
    "t_u = [35.7, 55.9, 58.2, 81.9, 56.3, 48.9, 33.9, 21.8, 48.4, 60.4, 68.4]\n",
    "t_c = torch.tensor(t_c).unsqueeze(1) # <1>\n",
    "t_u = torch.tensor(t_u).unsqueeze(1) # <1>\n",
    "\n",
    "t_u.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "linear_model = nn.Linear(1,1)\n",
    "optimizer = optim.SGD(\n",
    "    linear_model.parameters(), # 기존 params 자리\n",
    "    lr = 1e-2\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "data": {
      "text/plain": "[Parameter containing:\n tensor([[0.1060]], requires_grad=True),\n Parameter containing:\n tensor([-0.6616], requires_grad=True)]"
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(linear_model.parameters())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "def training_loop(n_epochs, optimizer, model, loss_fn, t_u_train, t_u_val,\n",
    "                  t_c_train, t_c_val):\n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        t_p_train = model(t_u_train)\n",
    "        loss_train = loss_fn(t_p_train, t_c_train)\n",
    "\n",
    "        # 왜 valid 셋에 nograd 안하지?\n",
    "        t_p_val = model(t_u_val)\n",
    "        loss_val = loss_fn(t_p_val, t_c_val)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss_train.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if epoch == 1 or epoch % 1000 == 0:\n",
    "            print(f\"Epoch {epoch}, Training loss {loss_train.item():.4f},\"\n",
    "                  f\" Validation loss {loss_val.item():.4f}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Training loss 208.1362, Validation loss 91.1518\n",
      "Epoch 1000, Training loss 4.4426, Validation loss 2.5459\n",
      "Epoch 2000, Training loss 2.6901, Validation loss 4.2043\n",
      "Epoch 3000, Training loss 2.5447, Validation loss 5.7748\n",
      "\n",
      "Parameter containing:\n",
      "tensor([[5.6493]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-18.7451], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "training_loop(\n",
    "    n_epochs=3000,\n",
    "    optimizer=optimizer,\n",
    "    model=linear_model,\n",
    "    loss_fn=nn.MSELoss(),\n",
    "    t_u_train=t_un_train,\n",
    "    t_u_val=t_un_val,\n",
    "    t_c_val=t_c_val,\n",
    "    t_c_train=t_c_train,\n",
    ")\n",
    "print()\n",
    "print(linear_model.weight)\n",
    "print(linear_model.bias)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "이전 챕터에서 한 훈련 방식을 파이토치의 모듈로 간단하게 표현하고 진행했다. 지금부터는 선형 함수 모델에 활성 함수를 씌워 간단한 신경망을 만들어보도록 한다.\n",
    "\n",
    "지금부터 만들 신경망의 아키텍처는 아래 그림과 같다. 기본적으로 신경망은 선형 모듈에 활성 함수를 씌운 층(layer, module)을 거친 값을 다음 선형 모듈에 넣는 방식이다.\n",
    "\n",
    "![](.notepad_images/241f3591.png)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}